"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ CSV —Ñ–∞–π–ª–æ–≤ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ data/.
"""
import pandas as pd
import numpy as np
from typing import Dict, List
from pathlib import Path
from sqlalchemy.orm import Session
from sqlalchemy import select, delete

from app.models.object import Object, ObjectType, LocationStatus
from app.models.diagnostic import Diagnostic, DiagnosticMethod, MLLabel, QualityGrade
from app.models.pipeline import Pipeline
from app.core.ml_model import ml_model
from app.core.logging_config import logger


def _determine_criticality_by_rules(
    defect_found: bool,
    defect_description: str,
    param1: float,
    param2: float,
    param3: float,
    method: str,
) -> str:
    """
    –ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–µ—Ñ–µ–∫—Ç–∞.
    
    Args:
        defect_found: –ù–∞–π–¥–µ–Ω –ª–∏ –¥–µ—Ñ–µ–∫—Ç
        defect_description: –û–ø–∏—Å–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞
        param1: –ü–∞—Ä–∞–º–µ—Ç—Ä 1 (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≥–ª—É–±–∏–Ω–∞ –∫–æ—Ä—Ä–æ–∑–∏–∏)
        param2: –ü–∞—Ä–∞–º–µ—Ç—Ä 2 (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–ª–æ—â–∞–¥—å –¥–µ—Ñ–µ–∫—Ç–∞)
        param3: –ü–∞—Ä–∞–º–µ—Ç—Ä 3 (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä)
        method: –ú–µ—Ç–æ–¥ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
    Returns:
        "normal", "medium" –∏–ª–∏ "high"
    """
    # –ï—Å–ª–∏ –¥–µ—Ñ–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω - –Ω–æ—Ä–º–∞
    if not defect_found:
        return "normal"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–∞
    desc_lower = defect_description.lower()
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    critical_keywords = [
        "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π", "–∫—Ä–∏—Ç–∏—á–Ω–æ", "–∞–≤–∞—Ä–∏–π–Ω—ã–π", "–∞–≤–∞—Ä–∏—è",
        "—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ", "—Ä–∞–∑—Ä—ã–≤", "—Ç—Ä–µ—â–∏–Ω–∞ —Å–∫–≤–æ–∑–Ω–∞—è", "—Å–∫–≤–æ–∑–Ω–∞—è",
        "–≥–ª—É–±–æ–∫–∞—è –∫–æ—Ä—Ä–æ–∑–∏—è", "—Å–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–æ–∑–∏—è", "–æ–±—à–∏—Ä–Ω–∞—è",
    ]
    
    # –°—Ä–µ–¥–Ω–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    medium_keywords = [
        "–∫–æ—Ä—Ä–æ–∑–∏—è", "–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ", "–¥–µ—Ñ–µ–∫—Ç", "—Ç—Ä–µ—â–∏–Ω–∞",
        "–∏–∑–Ω–æ—Å", "–∏–∑–Ω–æ—à–µ–Ω", "–ø–æ–≤—Ä–µ–∂–¥–µ–Ω",
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    is_critical = any(keyword in desc_lower for keyword in critical_keywords)
    is_medium = any(keyword in desc_lower for keyword in medium_keywords)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # param1 –æ–±—ã—á–Ω–æ –≥–ª—É–±–∏–Ω–∞/—Ä–∞–∑–º–µ—Ä –¥–µ—Ñ–µ–∫—Ç–∞
    # param2 –æ–±—ã—á–Ω–æ –ø–ª–æ—â–∞–¥—å/–ø—Ä–æ—Ç—è–∂–µ–Ω–Ω–æ—Å—Ç—å
    # param3 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–µ—Ç–æ–¥–∞, –Ω–æ –æ–±—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞)
    critical_param1_threshold = 20.0  # –º–º –¥–ª—è –∫–æ—Ä—Ä–æ–∑–∏–∏
    critical_param2_threshold = 50.0  # % –ø–ª–æ—â–∞–¥–∏
    medium_param1_threshold = 10.0
    medium_param2_threshold = 20.0
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_criticality = "normal"
    
    if param1 >= critical_param1_threshold or param2 >= critical_param2_threshold:
        param_criticality = "high"
    elif param1 >= medium_param1_threshold or param2 >= medium_param2_threshold:
        param_criticality = "medium"
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if is_critical or param_criticality == "high":
        return "high"
    elif is_medium or param_criticality == "medium":
        return "medium"
    else:
        return "normal"


def _train_model_sync(session: Session) -> Dict:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î.
    
    Args:
        session: DB —Å–µ—Å—Å–∏—è
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    """
    try:
        from app.core.config import settings
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        labeled_diagnostics = (
            session.query(Diagnostic, Object.year)
            .join(Object, Diagnostic.object_id == Object.id)
            .filter(Diagnostic.ml_label.isnot(None))
            .all()
        )
        
        min_samples = settings.ML_MIN_SAMPLES_FOR_TRAINING
        if len(labeled_diagnostics) < min_samples:
            logger.info(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {min_samples}, –ø–æ–ª—É—á–µ–Ω–æ {len(labeled_diagnostics)}")
            return {
                "trained": False,
                "samples": len(labeled_diagnostics),
            }
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = []
        for diagnostic, year in labeled_diagnostics:
            data.append({
                "param1": diagnostic.param1 or 0,
                "param2": diagnostic.param2 or 0,
                "method": diagnostic.method.value,
                "defect_found": diagnostic.defect_found,
                "object_year": year or 2000,
                "ml_label": diagnostic.ml_label.value,
            })
        
        df = pd.DataFrame(data)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
        X = df.drop(columns=["ml_label"])
        y = df["ml_label"].values
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        result = ml_model.train(X, y, use_mlflow=True)
        
        logger.info(f"‚úÖ ML –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(labeled_diagnostics)} –∑–∞–ø–∏—Å—è—Ö")
        return {
            "trained": True,
            "samples": len(labeled_diagnostics),
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
        return {
            "trained": False,
            "error": str(e),
        }


def _read_data_file(file_path: Path) -> pd.DataFrame:
    """
    –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö (CSV –∏–ª–∏ XLSX).
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    file_ext = file_path.suffix.lower()
    if file_ext == '.xlsx' or file_ext == '.xls':
        logger.info(f"–ß—Ç–µ–Ω–∏–µ XLSX —Ñ–∞–π–ª–∞: {file_path}")
        return pd.read_excel(file_path)
    else:
        logger.info(f"–ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞: {file_path}")
        return pd.read_csv(file_path)


def import_data_from_csv(
    session: Session,
    diagnostics_csv_path: Path,
    objects_csv_path: Path = None,
    clear_existing: bool = True,
) -> Dict:
    """
    –ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV/XLSX —Ñ–∞–π–ª–æ–≤.
    
    –ï—Å–ª–∏ objects_csv_path –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—Å—Ç –æ–±—ä–µ–∫—Ç—ã –∏–∑ Diagnostics.
    –≠—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å - —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –º–æ–∂–µ—Ç –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ç—á–µ—Ç –æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ,
    –∞ AI/ML —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—Å—Ç –æ–±—ä–µ–∫—Ç—ã –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ.
    
    Args:
        session: DB —Å–µ—Å—Å–∏—è
        diagnostics_csv_path: –ü—É—Ç—å –∫ Diagnostics.csv/xlsx (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        objects_csv_path: –ü—É—Ç—å –∫ Objects.csv/xlsx (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –µ—Å–ª–∏ None, –æ–±—ä–µ–∫—Ç—ã —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        clear_existing: –ï—Å–ª–∏ True, –æ—á–∏—â–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–º–ø–æ—Ä—Ç–∞
    """
    errors = []
    auto_created_objects = 0
    
    try:
        # –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
        # –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if clear_existing:
            logger.info("–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
            session.execute(delete(Diagnostic))
            session.execute(delete(Object))
            session.execute(delete(Pipeline))
            # –ù–µ –∫–æ–º–º–∏—Ç–∏–º –∑–¥–µ—Å—å, –≤—Å–µ –≤ –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        
        # –ß–∏—Ç–∞–µ–º Diagnostics –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è object_id (–Ω—É–∂–Ω–æ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ)
        logger.info("–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ Diagnostics...")
        diagnostics_df = _read_data_file(diagnostics_csv_path)
        
        # –ï—Å–ª–∏ Objects —Ñ–∞–π–ª –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ Diagnostics
        if objects_csv_path is None:
            logger.info("–§–∞–π–ª Objects –Ω–µ —É–∫–∞–∑–∞–Ω. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ Diagnostics...")
            
            # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ object_id –∏–∑ Diagnostics
            unique_object_ids = diagnostics_df["object_id"].dropna().unique()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã
            existing_object_ids = set()
            if not clear_existing:
                existing_objects = session.query(Object.object_id).all()
                existing_object_ids = {row[0] for row in existing_objects}
            
            # –°–æ–∑–¥–∞–µ–º Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
            default_pipeline_name = "AUTO-CREATED"
            default_pipeline = session.query(Pipeline).filter(Pipeline.name == default_pipeline_name).first()
            if not default_pipeline:
                default_pipeline = Pipeline(name=default_pipeline_name)
                session.add(default_pipeline)
                session.flush()
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç
            new_objects = []
            for obj_id in unique_object_ids:
                try:
                    obj_id_int = int(obj_id)
        
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
                    if obj_id_int in existing_object_ids:
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ None
                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (None, None) –æ–∑–Ω–∞—á–∞—é—Ç, —á—Ç–æ –æ–±—ä–µ–∫—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ,
                    # –Ω–æ –µ—â–µ –Ω–µ –∏–º–µ–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏ –Ω–µ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –Ω–∞ –∫–∞—Ä—Ç–µ
                    # –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ –±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
                    obj = Object(
                        object_id=obj_id_int,
                        object_name=f"–û–±—ä–µ–∫—Ç-{obj_id_int}",  # AI –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –∏–º—è –ø–æ–∑–∂–µ
                        object_type=ObjectType.PIPELINE_SECTION,  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–∏–ø
                        pipeline_id=default_pipeline.id,
                        lat=None,  # None –æ–∑–Ω–∞—á–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã - –æ–±—ä–µ–∫—Ç –Ω–µ –±—É–¥–µ—Ç –Ω–∞ –∫–∞—Ä—Ç–µ
                        lon=None,  # None –æ–∑–Ω–∞—á–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã - –æ–±—ä–µ–∫—Ç –Ω–µ –±—É–¥–µ—Ç –Ω–∞ –∫–∞—Ä—Ç–µ
                        location_status=LocationStatus.PENDING,  # –°—Ç–∞—Ç—É—Å: –æ–∂–∏–¥–∞–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                        year=None,
                        material=None,
                    )
                    new_objects.append(obj)
                except Exception as e:
                    errors.append(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ {obj_id}: {str(e)}")
            
            if new_objects:
                logger.info(f"‚ú® –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–æ {len(new_objects)} –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ Diagnostics (AI/ML –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ)...")
                session.add_all(new_objects)
                session.flush()
                auto_created_objects = len(new_objects)
            
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
            objects_df = pd.DataFrame(columns=["object_id", "pipeline_id"])
        else:
            # 1. –ò–º–ø–æ—Ä—Ç Pipeline (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ pipeline_id –≤ Objects.csv)
            logger.info("–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ Objects...")
            objects_df = _read_data_file(objects_csv_path)
        
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º Pipeline –∑–∞–ø–∏—Å–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ Objects)
        pipeline_map = {}  # pipeline_id -> Pipeline –æ–±—ä–µ–∫—Ç
        
        if not objects_df.empty and "pipeline_id" in objects_df.columns:
            unique_pipelines = objects_df["pipeline_id"].unique()
            
        for pipeline_name in unique_pipelines:
            pipeline_name = str(pipeline_name).strip()
            if not pipeline_name:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
            existing = session.query(Pipeline).filter(Pipeline.name == pipeline_name).first()
            if existing:
                pipeline_map[pipeline_name] = existing
            else:
                pipeline = Pipeline(name=pipeline_name)
                session.add(pipeline)
                session.flush()  # –ü–æ–ª—É—á–∞–µ–º ID –±–µ–∑ –∫–æ–º–º–∏—Ç–∞
                pipeline_map[pipeline_name] = pipeline
        
        # 2. –ò–º–ø–æ—Ä—Ç Objects (–µ—Å–ª–∏ —Ñ–∞–π–ª –±—ã–ª —É–∫–∞–∑–∞–Ω)
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ object_id –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        existing_object_ids = set()
        if not clear_existing:
            existing_objects = session.query(Object.object_id).all()
            existing_object_ids = {row[0] for row in existing_objects}
        
        objects = []
        skipped_objects = 0
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –±—ã–ª —É–∫–∞–∑–∞–Ω
        if objects_csv_path is not None and not objects_df.empty:
            for idx, row in objects_df.iterrows():
                try:
                    csv_object_id = int(row["object_id"])
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –Ω–µ –æ—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                    if not clear_existing and csv_object_id in existing_object_ids:
                        skipped_objects += 1
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –æ–±—ä–µ–∫—Ç
                    
                    pipeline_name = str(row["pipeline_id"]).strip()
                    pipeline = pipeline_map.get(pipeline_name)
                    if not pipeline:
                        errors.append(f"–°—Ç—Ä–æ–∫–∞ {idx + 2}: pipeline_id '{pipeline_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
                        continue
                    
                    object_type_str = str(row["object_type"]).lower()
                    if object_type_str not in ["crane", "compressor", "pipeline_section"]:
                        errors.append(f"–°—Ç—Ä–æ–∫–∞ {idx + 2}: –Ω–µ–≤–µ—Ä–Ω—ã–π object_type '{row['object_type']}'")
                        continue
                    
                    obj = Object(
                        object_id=csv_object_id,
                        object_name=str(row["object_name"]),
                        object_type=ObjectType(object_type_str),
                        pipeline_id=pipeline.id,
                        lat=float(row["lat"]),
                        lon=float(row["lon"]),
                        year=int(row["year"]) if pd.notna(row.get("year")) else None,
                        material=str(row["material"]) if pd.notna(row.get("material")) else None,
                    )
                    objects.append(obj)
                except Exception as e:
                    errors.append(f"–°—Ç—Ä–æ–∫–∞ {idx + 2}: {str(e)}")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–Ω–æ–≤—ã—Ö –∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö)
        object_id_map = {}
        db_objects = []
        
        if objects:
            logger.info(f"–ò–º–ø–æ—Ä—Ç {len(objects)} –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ø—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {skipped_objects})...")
            session.add_all(objects)
            session.flush()  # –ü–æ–ª—É—á–∞–µ–º ID –±–µ–∑ –∫–æ–º–º–∏—Ç–∞
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã (–Ω–æ–≤—ã–µ + —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ) –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        all_csv_object_ids = set()
        
        # –ï—Å–ª–∏ –±—ã–ª —Ñ–∞–π–ª Objects, —Å–æ–±–∏—Ä–∞–µ–º object_id –∏–∑ –Ω–µ–≥–æ
        if objects_csv_path is not None and not objects_df.empty:
            for _, row in objects_df.iterrows():
                try:
                    all_csv_object_ids.add(int(row["object_id"]))
                except:
                    pass
        
        # –ï—Å–ª–∏ –Ω–µ –æ—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ object_id –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
        if not clear_existing:
            all_csv_object_ids.update(existing_object_ids)
        
        # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º object_id –∏–∑ Diagnostics (–≤–∫–ª—é—á–∞—è –∞–≤—Çoc–æ–∑–¥–∞–Ω–Ω—ã–µ)
        if auto_created_objects > 0:
            unique_diag_object_ids = diagnostics_df["object_id"].dropna().unique()
            for obj_id in unique_diag_object_ids:
                try:
                    all_csv_object_ids.add(int(obj_id))
                except:
                    pass
        
        if all_csv_object_ids:
            db_objects = session.query(Object).filter(Object.object_id.in_(list(all_csv_object_ids))).all()
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ object_id (–∏–∑ CSV) -> Object.id (–≤ –ë–î)
            for obj in db_objects:
                object_id_map[obj.object_id] = obj.id
        
        # 3. –ò–º–ø–æ—Ä—Ç Diagnostics (—Ñ–∞–π–ª —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω –≤—ã—à–µ, –µ—Å–ª–∏ objects_csv_path –±—ã–ª None)
        if objects_csv_path is not None:
            diagnostics_df = _read_data_file(diagnostics_csv_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        source_file_objects = objects_csv_path.name if objects_csv_path else "AUTO-CREATED"
        source_file_diagnostics = diagnostics_csv_path.name
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ diag_id –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        existing_diag_ids = set()
        if not clear_existing:
            existing_diagnostics = session.query(Diagnostic.diag_id).all()
            existing_diag_ids = {row[0] for row in existing_diagnostics}
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ object_id -> year –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ML
        object_year_map = {obj.object_id: obj.year for obj in db_objects if obj.year}
        
        diagnostics = []
        diagnostics_for_ml = []  # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –±–µ–∑ ml_label –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        skipped_diagnostics = 0
        
        for idx, row in diagnostics_df.iterrows():
            try:
                csv_diag_id = int(row["diag_id"])
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –Ω–µ –æ—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                if not clear_existing and csv_diag_id in existing_diag_ids:
                    skipped_diagnostics += 1
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
                
                csv_object_id = int(row["object_id"])
                db_object_id = object_id_map.get(csv_object_id)
                
                if not db_object_id:
                    errors.append(f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä–æ–∫–∞ {idx + 2}: object_id {csv_object_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    continue
                
                method_str = str(row["method"]).upper()
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã –∏–∑ –¢–ó
                valid_methods = ["VIK", "PVK", "MPK", "UZK", "RGK", "TVK", "VIBRO", "MFL", "TFI", "GEO", "UTWM", "UT", "EC"]
                if method_str not in valid_methods:
                    errors.append(f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä–æ–∫–∞ {idx + 2}: –Ω–µ–≤–µ—Ä–Ω—ã–π method '{row['method']}'. –î–æ–ø—É—Å—Ç–∏–º—ã–µ: {', '.join(valid_methods)}")
                    continue
                
                date = pd.to_datetime(row["date"]).date()
                
                # –í–°–ï–ì–î–ê –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (ML –∏–ª–∏ –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
                diagnostics_for_ml.append({
                    "index": idx,
                    "row": row,
                    "db_object_id": db_object_id,
                    "method": method_str,
                    "date": date,
                    "object_year": object_year_map.get(csv_object_id),
                })
                
                # –ü–∞—Ä—Å–∏–º quality_grade
                quality_grade = None
                if pd.notna(row.get("quality_grade")) and str(row["quality_grade"]).strip():
                    quality_grade_str = str(row["quality_grade"]).strip().lower()
                    quality_grade_map = {
                        "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ": QualityGrade.–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û,
                        "–¥–æ–ø—É—Å—Ç–∏–º–æ": QualityGrade.–î–û–ü–£–°–¢–ò–ú–û,
                        "—Ç—Ä–µ–±—É–µ—Ç_–º–µ—Ä": QualityGrade.–¢–†–ï–ë–£–ï–¢_–ú–ï–†,
                        "–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ": QualityGrade.–ù–ï–î–û–ü–£–°–¢–ò–ú–û,
                    }
                    if quality_grade_str in quality_grade_map:
                        quality_grade = quality_grade_map[quality_grade_str]
                
                diag = Diagnostic(
                    diag_id=csv_diag_id,
                    object_id=db_object_id,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ID –∏–∑ –ë–î
                    method=DiagnosticMethod(method_str),
                    date=date,
                    temperature=float(row["temperature"]) if pd.notna(row.get("temperature")) else None,
                    humidity=float(row["humidity"]) if pd.notna(row.get("humidity")) else None,
                    illumination=float(row["illumination"]) if pd.notna(row.get("illumination")) else None,
                    defect_found=bool(row.get("defect_found", False)),
                    defect_description=str(row["defect_description"]) if pd.notna(row.get("defect_description")) else None,
                    quality_grade=quality_grade,
                    param1=float(row["param1"]) if pd.notna(row.get("param1")) else None,
                    param2=float(row["param2"]) if pd.notna(row.get("param2")) else None,
                    param3=float(row["param3"]) if pd.notna(row.get("param3")) else None,
                    ml_label=None,  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏
                    source_file=source_file_diagnostics,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                )
                diagnostics.append(diag)
            except Exception as e:
                errors.append(f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä–æ–∫–∞ {idx + 2}: {str(e)}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –í–°–ï–• –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫
        ml_predictions = {}
        if diagnostics_for_ml:
            try:
                logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –¥–ª—è {len(diagnostics_for_ml)} –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫...")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                ml_data = []
                for diag_info in diagnostics_for_ml:
                    row = diag_info["row"]
                    ml_data.append({
                        "method": diag_info["method"],
                        "param1": float(row["param1"]) if pd.notna(row.get("param1")) else 0.0,
                        "param2": float(row["param2"]) if pd.notna(row.get("param2")) else 0.0,
                        "param3": float(row["param3"]) if pd.notna(row.get("param3")) else 0.0,
                        "defect_found": bool(row.get("defect_found", False)),
                        "defect_description": str(row.get("defect_description", "")).lower() if pd.notna(row.get("defect_description")) else "",
                        "object_year": diag_info["object_year"] or 2000,
                    })
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º ML –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –æ–Ω–∞ –æ–±—É—á–µ–Ω–∞
                if ml_model.is_trained:
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É—é –æ–±—É—á–µ–Ω–Ω—É—é ML –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏")
                    ml_df = pd.DataFrame(ml_data)
                    features = ml_model.prepare_features(ml_df)
                    predictions = ml_model.predict(features)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    for i, diag_info in enumerate(diagnostics_for_ml):
                        pred_label = predictions[i]
                        if pred_label in ["normal", "medium", "high"]:
                            ml_predictions[diag_info["index"]] = MLLabel(pred_label)
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –ª–æ–≥–∏–∫—É, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞
                    logger.info("ML –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –ª–æ–≥–∏–∫—É")
                    for i, diag_info in enumerate(diagnostics_for_ml):
                        data = ml_data[i]
                        criticality = _determine_criticality_by_rules(
                            defect_found=data["defect_found"],
                            defect_description=data["defect_description"],
                            param1=data["param1"],
                            param2=data["param2"],
                            param3=data["param3"],
                            method=data["method"],
                        )
                        ml_predictions[diag_info["index"]] = MLLabel(criticality)
                
                logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(ml_predictions)} –º–µ—Ç–æ–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {e}", exc_info=True)
                errors.append(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {str(e)}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º ml_label –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ diag_id -> ml_label –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        diag_id_to_ml_label = {}
        for diag_info in diagnostics_for_ml:
            if diag_info["index"] in ml_predictions:
                diag_id = int(diagnostics_df.iloc[diag_info["index"]]["diag_id"])
                diag_id_to_ml_label[diag_id] = ml_predictions[diag_info["index"]]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞–º (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∞ –º–µ—Ç–∫–∞ –≤ CSV)
        for diag in diagnostics:
            if diag.diag_id in diag_id_to_ml_label:
                diag.ml_label = diag_id_to_ml_label[diag.diag_id]
                logger.debug(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å {diag.ml_label.value} –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ {diag.diag_id}")
        
        if diagnostics:
            logger.info(f"–ò–º–ø–æ—Ä—Ç {len(diagnostics)} –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫...")
            session.add_all(diagnostics)
            logger.info(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(diagnostics)} –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫, –∏–∑ –Ω–∏—Ö {len(ml_predictions)} —Å ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏")
        
        # –ö–æ–º–º–∏—Ç–∏–º –≤—Å—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –æ–¥–Ω–∏–º —Ä–∞–∑–æ–º
        logger.info("–ö–æ–º–º–∏—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏...")
        session.commit()
        logger.info("–ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å ML –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        train_result = {"trained": False, "samples": 0}
        if not ml_model.is_trained:
            logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏...")
            train_result = _train_model_sync(session)
            if train_result.get("trained"):
                logger.info(f"‚úÖ ML –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–µ–Ω–∞ –Ω–∞ {train_result.get('samples')} –∑–∞–ø–∏—Å—è—Ö")
            else:
                logger.info(f"‚ö†Ô∏è  ML –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({train_result.get('samples', 0)} –∑–∞–ø–∏—Å–µ–π)")
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ML –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenAI (–∫–∞–∂–¥—ã–π 10-–π –∏–º–ø–æ—Ä—Ç)
        try:
            from app.services.ml_monitor import monitor_and_improve
            import random
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º —Å–ª—É—á–∞–π–Ω–æ –≤ 10% —Å–ª—É—á–∞–µ–≤ –∏–ª–∏ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ —á—Ç–æ –æ–±—É—á–∏–ª–∞—Å—å
            if train_result.get("trained") or random.random() < 0.1:
                logger.info("–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ ML –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenAI...")
                monitor_result = monitor_and_improve(session, auto_improve=False)
                if monitor_result.get("ai_analysis"):
                    logger.info("‚úÖ OpenAI –∞–Ω–∞–ª–∏–∑ ML –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω")
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    for suggestion in monitor_result.get("suggestions", [])[:3]:  # –ü–µ—Ä–≤—ã–µ 3
                        logger.info(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {suggestion.get('message', '')}")
        except Exception as e:
            logger.debug(f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ML —á–µ—Ä–µ–∑ OpenAI –ø—Ä–æ–ø—É—â–µ–Ω: {e}")
        
        return {
            "success": True,
            "pipelines_imported": len(pipeline_map),
            "objects_imported": len(objects),
            "objects_auto_created": auto_created_objects,
            "objects_skipped": skipped_objects,
            "diagnostics_imported": len(diagnostics),
            "diagnostics_skipped": skipped_diagnostics,
            "ml_predictions_made": len(ml_predictions),
            "errors": errors,
        }
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ: {e}", exc_info=True)
        session.rollback()
        logger.error("–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –æ—Ç–∫–∞—á–µ–Ω–∞")
        return {
            "success": False,
            "error": str(e),
            "errors": errors,
        }
